import os
import json
import asyncio
import re
import csv
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from deepgram import Deepgram
from deepgram_captions import DeepgramConverter
from pydub import AudioSegment

# Load environment variables
load_dotenv()

# Get the Deepgram API key from environment variables
DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY")
dg_client = Deepgram(DEEPGRAM_API_KEY)

# Define directories (adjust these as needed)
AUDIO_DIR_PATH = "/home/eleven/makeADataset/raw_Audio"
JSON_DIR_PATH = "/home/eleven/deepgram/jsons"
SRT_OUTPUT_DIR = "/home/eleven/deepgram/converted_SRTs"
WAVS_DIR = "/home/eleven/deepgram/segments"
PARENT_CSV = "/home/eleven/deepgram"
SPEAKER_NAME = "Speaker"
TRAIN_CSV = "/home/eleven/deepgram/train_metadata.csv"
EVAL_CSV = "/home/eleven/deepgram/eval_metadata.csv"

# Create directories if they don't exist
os.makedirs(JSON_DIR_PATH, exist_ok=True)
os.makedirs(SRT_OUTPUT_DIR, exist_ok=True)
os.makedirs(WAVS_DIR, exist_ok=True)

# Transcription options
options = {
    "model": "whisper-large",
    "punctuate": True,
    "utterances": True,
    "paragraphs": True,
    "smart_format": True,
    "filler_words": True
}

# 1. Transcribe audio files and save as JSON
async def transcribe_audio(file_path):
    try:
        with open(file_path, "rb") as audio_file:
            audio_source = {"buffer": audio_file, "mimetype": "audio/wav"}
            response = await dg_client.transcription.prerecorded(audio_source, options)

            base_name = os.path.splitext(os.path.basename(file_path))[0]
            json_file_name = f"{base_name}.json"
            json_file_path = os.path.join(JSON_DIR_PATH, json_file_name)

            with open(json_file_path, "w") as json_file:
                json.dump(response, json_file, indent=2)

            print(f"Transcription saved to {json_file_path}")

    except Exception as e:
        print(f"An error occurred while processing {file_path}: {e}")

async def transcribe_all_audio():
    audio_files = [os.path.join(AUDIO_DIR_PATH, f) for f in os.listdir(AUDIO_DIR_PATH) if f.endswith('.wav')]
    for audio_file in audio_files:
        await transcribe_audio(audio_file)

# 2. Generate SRT files from the transcriptions
def gaussian_duration(min_duration, max_duration):
    mean_duration = (min_duration + max_duration) / 2
    std_duration = (max_duration - min_duration) / 4
    duration = np.random.normal(mean_duration, std_duration)
    return max(min_duration, min(duration, max_duration))

def format_time(seconds):
    milliseconds = int((seconds - int(seconds)) * 1000)
    return f"{int(seconds // 3600):02}:{int((seconds % 3600) // 60):02}:{int(seconds % 60):02},{milliseconds:03}"

def generate_srt(captions):
    srt_content = ""
    for i, (start, end, text) in enumerate(captions, 1):
        srt_content += f"{i}\n{format_time(start)} --> {format_time(end)}\n{text}\n\n"
    return srt_content

def process_transcription(json_path):
    with open(json_path, 'r') as f:
        dg_response = json.load(f)
    transcription = DeepgramConverter(dg_response)

    line_length = 250
    lines = transcription.get_lines(line_length)

    captions = []
    for line_group in lines:
        for line in line_group:
            start_time = line.get('start')
            end_time = line.get('end')
            text = line.get('punctuated_word')
            if start_time is not None and end_time is not None and text is not None:
                captions.append((start_time, end_time, text))

    processed_captions = []
    current_start, current_end, current_text = None, None, ""
    segment_duration = gaussian_duration(1.2, 15)

    for start, end, text in captions:
        if current_start is None:
            current_start = start
            current_end = end
            current_text = text
        elif end - current_start <= segment_duration and len(current_text + " " + text) <= 250:
            current_end = end
            current_text += " " + text
        else:
            if current_end - current_start >= 1.2:
                processed_captions.append((current_start, current_end, current_text.strip()))
            current_start = start
            current_end = end
            current_text = text
            segment_duration = gaussian_duration(1.2, 15)

    if current_end - current_start >= 1.2:
        processed_captions.append((current_start, current_end, current_text.strip()))

    return processed_captions

def generate_srt_files():
    json_files = [f for f in os.listdir(JSON_DIR_PATH) if f.endswith('.json')]
    for json_file in json_files:
        base_name = os.path.splitext(json_file)[0]
        json_path = os.path.join(JSON_DIR_PATH, json_file)
        processed_captions = process_transcription(json_path)
        if processed_captions:
            srt_content = generate_srt(processed_captions)
            srt_file_path = os.path.join(SRT_OUTPUT_DIR, f"{base_name}.srt")
            with open(srt_file_path, "w") as srt_file:
                srt_file.write(srt_content)
            print(f"SRT saved to {srt_file_path}")

# 3. Segment audio based on SRT and create metadata CSV
def srt_time_to_ms(srt_time):
    hours, minutes, seconds, milliseconds = map(int, re.split('[:,]', srt_time))
    return (hours * 3600 + minutes * 60 + seconds) * 1000 + milliseconds

def segment_audio_and_create_metadata():
    CSV_FILE_PATH = os.path.join(PARENT_CSV, "metadata.csv")
    with open(CSV_FILE_PATH, 'w', newline='') as csv_file:
        csv_writer = csv.writer(csv_file, delimiter='|')
        csv_writer.writerow(['audio', 'text', 'speaker_name'])

        srt_files = [f for f in os.listdir(SRT_OUTPUT_DIR) if f.endswith('.srt')]
        for srt_file in srt_files:
            base_name = os.path.splitext(srt_file)[0]
            audio_file = f"{base_name}.wav"
            audio_path = os.path.join(AUDIO_DIR_PATH, audio_file)
            srt_path = os.path.join(SRT_OUTPUT_DIR, srt_file)

            if os.path.exists(audio_path):
                audio = AudioSegment.from_wav(audio_path)
                with open(srt_path, 'r') as srt_file:
                    srt_content = srt_file.read()

                segments = re.findall(r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n(.*?)\n\n', srt_content, re.DOTALL)
                for segment in segments:
                    index, start_time, end_time, text = segment
                    start_ms = srt_time_to_ms(start_time)
                    end_ms = srt_time_to_ms(end_time)

                    audio_segment = audio[start_ms:end_ms]
                    segment_filename = f"{base_name}_segment_{index}.wav"
                    segment_filepath = os.path.join(WAVS_DIR, segment_filename)
                    audio_segment.export(segment_filepath, format="wav")

                    full_audio_path = os.path.abspath(segment_filepath)
                    csv_writer.writerow([full_audio_path, text, SPEAKER_NAME])

# 4. Split dataset into training and evaluation sets
def split_dataset(eval_percentage=20):
    input_file_path = os.path.join(PARENT_CSV, "metadata.csv")
    train_df = pd.read_csv(input_file_path, delimiter="|")
    num_rows_to_move = int(len(train_df) * eval_percentage / 100)

    rows_to_move = train_df.sample(n=num_rows_to_move, random_state=42)
    train_df = train_df.drop(rows_to_move.index)
    eval_df = rows_to_move

    train_df.to_csv(TRAIN_CSV, sep="|", index=False)
    eval_df.to_csv(EVAL_CSV, sep="|", index=False)
    print(f"Moved {num_rows_to_move} rows to evaluation dataset.")

# Main entry point
def main():
    asyncio.run(transcribe_all_audio())
    generate_srt_files()
    segment_audio_and_create_metadata()
    split_dataset()

if __name__ == "__main__":
    main()
